# Backend System

The backend system is the execution engine of the replay framework. It processes the intermediate representation (IR) generated by the prompt preprocessor and executes the workflow by dispatching operations to specialized node processors.

## Architecture Overview

The backend follows a **processor pattern** where different types of operations (opcodes) are handled by dedicated processor classes. This design provides:

- **Modularity**: Each operation type has its own processor
- **Extensibility**: New operations can be added by creating new processors
- **Maintainability**: Logic for each operation is isolated and testable
- **Flexibility**: Processors can be swapped or modified independently

## Core Components

### 1. Replay Engine (`replay.py`)

The central orchestrator that manages the execution lifecycle:

```python
# Create new replay from a prompt file
replay = Replay.from_recipe(
    InputConfig(
        input_prompt_file="path/to/prompt.txt",
        project_name="my_project"
    )
)

# Execute the entire workflow
replay.run_all()

# Or execute step by step
while replay.has_steps():
    replay.run_step()
```

**Key Responsibilities:**
- Compiles prompt files into executable IR graphs
- Manages project directories and versioning
- Coordinates LLM client interactions
- Saves execution state for checkpoints
- Handles control flow through the IR graph

**Directory Structure:**
```
project_dir/
├── latest/           # Symlink to latest version
├── 1/               # Version 1
│   ├── code/        # Generated/modified code
│   └── replay/      # Execution metadata
│       ├── docs/    # Referenced documentation
│       ├── template/# Template files
│       ├── run_logs/# Command execution logs
│       └── client/  # LLM request/response logs
└── 2/               # Version 2
    └── ...
```

### 2. Node Processor Registry (`processors/registry.py`)

Manages the mapping between operation types and their processors:

```python
registry = NodeProcessorRegistry.create_registry()
processor = registry.get(Opcode.PROMPT)
processor.process(replay, node)
```

### 3. Specialized Processors (`processors/`)

Each processor handles a specific type of operation:

#### PromptNodeProcessor
- Sends prompts to LLM with context files
- Processes file references (@docs:, @template:, @code:)
- Saves generated code to the working directory
- Manages conversation memory

#### RunNodeProcessor  
- Executes shell commands in the code directory
- Captures stdout/stderr to timestamped log files
- Records exit codes for conditional branching
- Updates replay memory with execution results

#### FixNodeProcessor
- Analyzes failed command outputs
- Identifies relevant code files mentioned in logs
- Sends error analysis requests to LLM
- Applies suggested fixes to code files

#### ConditionalNodeProcessor
- Evaluates conditions based on command exit codes
- Manages iteration counters for retry loops
- Directs control flow to true/false branches

#### TemplateNodeProcessor & DocNodeProcessor
- Handle copying of template and documentation files
- Set up initial project structure

#### ExitNodeProcessor
- Marks workflow completion
- Triggers cleanup and finalization

### 4. LLM Client System (`client/`)

Provides abstraction over LLM interactions:

#### ClientWrapper (`client_wrapper.py`)
- Wraps Anthropic client with logging
- Saves all requests/responses for debugging
- Provides timestamped interaction history

#### MockAnthropicClient (`mock_anthropic.py`)
- Provides testing capabilities without real LLM calls
- Captures request structure for validation
- Returns mock responses for development

## Execution Flow

1. **Initialization**
   - Set up project directories
   - Initialize LLM client (real or mock)
   - Copy system instructions

2. **Compilation**
   - Parse prompt file into IR graph
   - Apply preprocessing passes
   - Copy referenced files (templates, docs)

3. **Execution**
   - Start from first node in graph
   - Process each node with appropriate processor
   - Follow control flow edges
   - Save state after each step

4. **Completion**
   - Update latest symlink
   - Save final state
   - Clean up temporary files

## State Management

The replay system maintains several types of state:

### ReplayState
- **InputConfig**: Original prompt file and project settings
- **ExecutionState**: Runtime state (current node, memory, loaded IR)
- **ReplayStatus**: Current execution phase
- **Version**: Project version number

### ExecutionState
- **current_node_id**: Next node to execute
- **control_flow_graph_queue**: Control flow traversal state
- **epic**: Loaded IR graph
- **memory**: Conversation history for LLM context

States are automatically saved to `replay_state.json` and can be used to resume execution from checkpoints.

## Configuration

### System Prompts (`system_prompts/`)
- **client_instructions_with_json.txt**: Main LLM system prompt
- **client_instructions_identify_issue.txt**: Error analysis prompt

### Environment Variables
- **ANTHROPIC_API_KEY**: Required for LLM interactions (unless using mock)

## Usage Examples

### Basic Execution
```python
from core.backend.replay import Replay, InputConfig

# New project
config = InputConfig(
    input_prompt_file="examples/prompts/my_prompt.txt",
    project_name="test_project",
    output_dir="./output"
)
replay = Replay.from_recipe(config)
replay.run_all()
```

### Checkpoint Resume
```python
# Resume from checkpoint
replay = Replay.load_checkpoint(
    project_name="test_project",
    version="latest",  # or specific version number
    output_dir="./output"
)
replay.run_all()
```

### Step-by-Step Execution
```python
replay = Replay.from_recipe(config)

while replay.has_steps():
    print(f"Processing: {replay.get_execution_state().current_node_id}")
    replay.run_step()
    replay.save_state()  # Save after each step
```

### Mock Mode for Testing
```python
replay = Replay.from_recipe(config, use_mock=True)
replay.run_all()

# Access captured requests for testing
mock_client = replay.client.client
print(mock_client.captured_jsons)
```

## Error Handling

The system provides comprehensive error handling:

- **Command Failures**: Captured in run logs, trigger FIX node processing
- **LLM Errors**: JSON parsing errors logged with full context
- **File Errors**: Missing files logged with helpful error messages
- **State Corruption**: Validation on state loading/saving

## Debugging

### Execution Logs
- All processor actions logged with timestamps
- Command outputs saved to timestamped files
- LLM requests/responses saved for analysis

### State Inspection
```python
# Get current execution state
state = replay.get_execution_state()
print(f"Current node: {state.current_node_id}")
print(f"Memory: {state.memory}")

# Get full program state
program = replay.get_program()
print(f"Graph nodes: {list(program.epic.graph.nodes())}")
```

## Extending the System

### Adding New Processors

1. Create processor class:
```python
class MyNodeProcessor:
    def process(self, replay, node: dict) -> None:
        # Process the node
        pass
```

2. Register in `registry.py`:
```python
registry.register(Opcode.MY_OPCODE, MyNodeProcessor())
```

### Custom System Prompts

Modify files in `system_prompts/` to customize LLM behavior:
- Change JSON schema requirements
- Add domain-specific instructions
- Modify error analysis approaches

## File Organization

```
core/backend/
├── README.md                          # This file
├── __init__.py                        # Package exports
├── replay.py                          # Main replay engine
├── client/                           # LLM client abstraction
│   ├── __init__.py
│   ├── client_wrapper.py             # Request/response logging
│   └── mock_anthropic.py             # Testing mock
├── processors/                       # Node processors
│   ├── __init__.py
│   ├── registry.py                   # Processor registry
│   ├── prompt_node_processor.py      # LLM interactions
│   ├── run_node_processor.py         # Command execution
│   ├── fix_node_processor.py         # Error analysis/fixing
│   ├── conditional_node_processor.py # Control flow
│   ├── template_node_processor.py    # Template handling
│   ├── doc_node_processor.py         # Documentation handling
│   └── exit_node_processor.py        # Workflow completion
└── system_prompts/                   # LLM system prompts
    ├── client_instructions_with_json.txt
    └── client_instructions_identify_issue.txt
``` 