{
  "content": [
    {
      "text": "{\n  \"files\": [\n    {\n      \"path_and_filename\": \"tt_eltwise_sfpu.cpp\",\n      \"contents\": \"// SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.\\n//\\n// SPDX-License-Identifier: Apache-2.0\\n\\n#include <cstdint>\\n#include \\\"compute_kernel_api/common.h\\\"\\n#include \\\"compute_kernel_api/tile_move_copy.h\\\"\\n#include \\\"compute_kernel_api/eltwise_unary/acos.h\\\"\\n#include \\\"compute_kernel_api/eltwise_unary/eltwise_unary.h\\\"\\n#include \\\"compute_kernel_api/eltwise_unary/sfpu_split_includes.h\\\"\\n#include \\\"compute_kernel_api.h\\\"\\n\\nnamespace NAMESPACE {\\nvoid MAIN {\\n    uint32_t per_core_block_cnt = get_compile_time_arg_val(0);\\n    uint32_t per_core_block_dim = get_compile_time_arg_val(1);\\n\\n    init_sfpu(tt::CBIndex::c_0, tt::CBIndex::c_16);\\n    for (uint32_t block_index = 0; block_index < per_core_block_cnt; block_index++) {\\n        cb_reserve_back(tt::CBIndex::c_16, per_core_block_dim);\\n        for (uint32_t tile_index = 0; tile_index < per_core_block_dim; ++tile_index) {\\n            tile_regs_acquire();\\n\\n            // Pop tile after tile, copy to DST and pack\\n            cb_wait_front(tt::CBIndex::c_0, 1);\\n            copy_tile(tt::CBIndex::c_0, 0, 0);\\n\\n            acos_tile_init(); \\n            acos_tile(0);\\n\\n            tile_regs_commit();\\n            tile_regs_wait();\\n            pack_tile(0, tt::CBIndex::c_16);\\n\\n            cb_pop_front(tt::CBIndex::c_0, 1);\\n            tile_regs_release();\\n        }\\n        cb_push_back(tt::CBIndex::c_16, per_core_block_dim);\\n    }\\n}\\n}  // namespace NAMESPACE\\n\"\n    },\n    {\n      \"path_and_filename\": \"test_newop.py\",\n      \"contents\": \"import torch\\nimport pytest\\nimport ttnn\\nimport numpy as np\\nimport os\\nfrom loguru import logger\\n\\ndef kernel_path(kernel_file_name):    \\n    current_file_path = os.path.abspath(__file__)\\n    current_dir = os.path.dirname(current_file_path)\\n    kernel_dir = current_dir\\n    return os.path.join(kernel_dir, kernel_file_name)\\n\\ndef prepare_newop_program(device, input_tensor, output_tensor, num_tiles, src_bank_id, dst_bank_id):\\n    core = ttnn.CoreCoord(0, 0)\\n    core_grid = ttnn.CoreRangeSet([ttnn.CoreRange(core, core)])\\n\\n    input_cb_data_format = ttnn.bfloat16  # this will be mapped tt::DataFormat::Float16_b\\n    cb_total_size = 2 * 2 * 1024  # tt::DataFormat::Float16_b hard coded to have size 2 * 1024\\n    cb_page_size = 2 * 1024\\n\\n    in_cb = 0\\n    out_cb = 16\\n    in_cb_format = ttnn.CBFormatDescriptor(buffer_index=in_cb, data_format=input_cb_data_format, page_size=cb_page_size,)\\n    out_cb_format = ttnn.CBFormatDescriptor(buffer_index=out_cb, data_format=input_cb_data_format, page_size=cb_page_size,)\\n    in_cb_descriptor = ttnn.CBDescriptor(total_size=cb_total_size,core_ranges=core_grid,format_descriptors=[in_cb_format],)\\n    out_cb_descriptor = ttnn.CBDescriptor(total_size=cb_total_size,core_ranges=core_grid,format_descriptors=[out_cb_format],)\\n\\n    is_dram_input = 1\\n    reader_compile_time_args = [is_dram_input]\\n    writer_compile_time_args = [out_cb, is_dram_input]\\n    compute_compile_time_args = [num_tiles, 1]\\n    reader_rt_args = [input_tensor.buffer_address(), num_tiles, 0]\\n    writer_rt_args = [output_tensor.buffer_address(), num_tiles, 0]\\n\\n\\n    # create a kernel descriptor for the reader kernel\\n    reader_kernel_descriptor = ttnn.KernelDescriptor(\\n        kernel_source=kernel_path(\\\"tt_reader.cpp\\\"),\\n        core_ranges=core_grid,\\n        compile_time_args=reader_compile_time_args,\\n        runtime_args=[[reader_rt_args]],\\n        config=ttnn.ReaderConfigDescriptor(),\\n    )\\n    writer_kernel_descriptor = ttnn.KernelDescriptor(\\n        kernel_source=kernel_path(\\\"tt_writer.cpp\\\"),\\n        core_ranges=core_grid,\\n        compile_time_args=writer_compile_time_args,\\n        runtime_args=[[writer_rt_args]],\\n        config=ttnn.WriterConfigDescriptor(),\\n    )\\n    \\n    compute_kernel_descriptor = ttnn.KernelDescriptor(\\n        kernel_source=kernel_path(\\\"tt_eltwise_sfpu.cpp\\\"),\\n        core_ranges=core_grid,\\n        compile_time_args=compute_compile_time_args,\\n        defines=[],\\n        runtime_args=[[[]]],\\n        config=ttnn.ComputeConfigDescriptor(),\\n    )\\n\\n    program_descriptor = ttnn.ProgramDescriptor(\\n        kernels=[reader_kernel_descriptor, writer_kernel_descriptor, compute_kernel_descriptor],\\n        semaphores=[],\\n        cbs=[in_cb_descriptor, out_cb_descriptor],\\n    )\\n\\n    return program_descriptor\\n\\ndef test_new_op():\\n    device = ttnn.open_device(device_id=0)\\n\\n    num_tiles = 4\\n    src_bank_id = 0\\n    dst_bank_id = 0\\n\\n    shape = [1, num_tiles, 32, 32]\\n    data = torch.rand(shape).to(torch.bfloat16)\\n\\n    dram_memory_config = ttnn.DRAM_MEMORY_CONFIG\\n\\n    input_tensor = ttnn.from_torch(data, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device, memory_config=dram_memory_config)\\n    output_tensor = ttnn.allocate_tensor_on_device(ttnn.Shape(shape), ttnn.bfloat16, ttnn.TILE_LAYOUT, device, dram_memory_config)\\n    \\n    io_tensors = [input_tensor, output_tensor]\\n\\n    program_descriptor = prepare_newop_program(device, input_tensor, output_tensor, num_tiles, src_bank_id, dst_bank_id)\\n\\n    output = ttnn.generic_op(io_tensors, program_descriptor)\\n    golden = ttnn.acos(input_tensor)\\n\\n    torch_golden = ttnn.to_torch(golden)\\n    torch_output = ttnn.to_torch(output)\\n    logger.info(f\\\"input_tensor: {input_tensor}\\\")\\n    logger.info(f\\\"torch_golden: {torch_golden}\\\")\\n    logger.info(f\\\"torch_output: {torch_output}\\\")\\n\\n    matching = torch.allclose(torch_golden, torch_output)\\n    logger.info(f\\\"Tensors are matching: {matching}\\\")\\n    assert matching\\n\\n    ttnn.close_device(device)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    test_new_op()\"\n    }\n  ],\n  \"commands_to_run\": [\n    \"python3 test_newop.py\"\n  ],\n  \"memory\": [\n    \"Updated tt_eltwise_sfpu.cpp to implement acos operation instead of exp by changing includes and function calls to use acos_tile_init and acos_tile instead of exp functions. Also added compute_kernel_api.h header.\",\n    \"Modified test_newop.py to compare against ttnn.acos instead of ttnn.exp to verify the acos implementation.\"\n  ]\n}"
    }
  ],
  "model": "claude-3-7-sonnet-20250219",
  "usage": {
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "input_tokens": 27726,
    "output_tokens": 2285,
    "server_tool_use": null,
    "service_tier": "standard"
  },
  "timestamp": "2025-07-03T03:47:54.166177"
}